Environment Overview â€” Spark on OpenShift (DEV-RNS)
ğŸ§± Cluster & Platform

Youâ€™re running on OpenShift 4.x (Red Hat) with Istio (Maistra 2.6.6).

You access via a bastion host and use oc + helm for administration.

Your cluster runs multiple application projects (namespaces), including:

dev-rns â†’ main app namespace (where Spark jobs run)

dev-spark-operator â†’ Spark Operator controller namespace

dev-spark â†’ legacy / supporting chart or service deployment

dev-admin â†’ admin-level services like admin-adminservice

âš™ï¸ Spark Deployment Architecture
Component	Namespace	Description	Status
Spark Operator (Helm-based)	dev-spark-operator	Helm release spark-operator, image registry.dev.sbiepay.sbi:8443/apache/spark-kubernetes-operator:0.4.0	âœ… Running
Watched Namespace(s)	dev-rns	Operator only reconciles jobs in dev-rns	âœ… Confirmed via logs
Spark History Server	dev-spark-operator	ClusterIP service spark-history-server:18080	âœ… Active
Spark CRDs	Cluster-wide	sparkapplications.spark.apache.org & sparkclusters.spark.apache.org	âœ… Installed
Driver/Executor Jobs	dev-rns	Created dynamically by Operator on SparkApplication submission	âœ… Working
Job Lifecycle	Ephemeral	Driver pods created â†’ run â†’ terminated after job completion	âœ… Expected behavior
ğŸ”’ Security, RBAC & SCC
Resource	Namespace	Purpose	Status
spark-operator	dev-spark-operator	Operator SA	âœ…
spark-sa	dev-rns	Used by driver/executor pods	âœ…
spark-scc-binding	ClusterRoleBinding	Binds SCC â†’ SA	âœ…
spark-scc	SCC	Custom security policy for Spark pods	âœ…
spark-operator-clusterrolebinding	ClusterRoleBinding	Grants operator CRD control	âœ…
SCC Privileges		Driver pods have necessary volumes + runAsAny	âœ…
ğŸ§© Applications Using Spark

You have two major app deployments (Helm-managed):

App	Namespace	Role	Interaction with Spark
ops-operationsservice	dev-rns	Main backend service	Submits Spark jobs (Recon etc.)
refund-refundservice	dev-rns	Companion service	Likely interacts with output / job triggers
Recon Spark Jobs	dev-rns	via recon-spark-app.yaml	Processed by Operator, driver/executor pods auto-created

These services interact with Spark via REST or internal Java clients, not manual spark-submit.

ğŸ§  Behavioral Notes

When a developer or backend service (like ops-operationsservice) submits a Spark job, the SparkApplication CRD is created.

The Spark Operator in dev-spark-operator reconciles it â†’ launches driver/executor pods â†’ updates CRD status.

Once the job finishes â†’ the driver & executors are terminated automatically.

Logs are pushed to the Spark History Server.

Prometheus-compatible metrics are exposed from the operator (/metrics endpoint on port 19091).

ğŸ“Š Current Observations

âœ… Operator stable for 70+ days
âœ… Health and readiness probes OK
âœ… Metrics service up
âœ… CRDs healthy
âš ï¸ Kerberos warning â€” irrelevant since youâ€™re using S3, Oracle JDBC
âœ… RBAC bindings valid
âœ… Spark jobs complete and clean up correctly
âœ… Istio sidecar injection active (2.6.6)
âœ… Helm charts managing everything cleanly

ğŸ§© Your Setup Type (Summary)

Youâ€™re running Spark-on-Kubernetes Operator pattern, not standalone Spark.

âœ” No manual spark-submit

âœ” Jobs launched via CRD (SparkApplication)

âœ” Operator handles driver lifecycle

âœ” History server manages logs

âœ” SCC & RBAC customized for OpenShift security

âœ” Istio-compatible (mesh injected pods)

âœ” All managed via Helm

ğŸ§° What We Can Do Next

If you confirm, I can now:

ğŸ§¾ Generate a diagnostic collector (single shell script) â€” will gather:

SparkApplications (status, duration, exit reasons)

Operator logs (last 500 lines)

SCC + RoleBindings summary

Active driver/executor pod details

History Server service check
â†’ Output as /tmp/spark_dev_rns_diagnostic.txt

ğŸ“Š Build a visual topology diagram showing:

Operator â†’ CRD â†’ Driver Pod â†’ Executor Pods

Namespaces and network flow (with Istio mesh)

âš¡ Draft a production-readiness checklist for Spark Operator on OpenShift (security, scaling, monitoring).
